{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-28T02:27:44.203493Z","iopub.execute_input":"2021-07-28T02:27:44.204101Z","iopub.status.idle":"2021-07-28T02:27:44.220664Z","shell.execute_reply.started":"2021-07-28T02:27:44.203976Z","shell.execute_reply":"2021-07-28T02:27:44.219771Z"}}},{"cell_type":"code","source":"import torch\nimport transformers\nimport pandas as pd\nimport numpy as np\nimport torch\nimport os\nimport sys\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score, precision_recall_curve, auc","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:25:37.444156Z","iopub.execute_input":"2021-07-28T03:25:37.444502Z","iopub.status.idle":"2021-07-28T03:25:39.620327Z","shell.execute_reply.started":"2021-07-28T03:25:37.444473Z","shell.execute_reply":"2021-07-28T03:25:39.619504Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModel\nfrom transformers import AutoTokenizer\n#seyonec/PubChem10M_SMILES_BPE_450k\nfrom transformers import RobertaModel,BertModel","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:25:39.621635Z","iopub.execute_input":"2021-07-28T03:25:39.621972Z","iopub.status.idle":"2021-07-28T03:25:43.966717Z","shell.execute_reply.started":"2021-07-28T03:25:39.621922Z","shell.execute_reply":"2021-07-28T03:25:43.965923Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"model = RobertaModel.from_pretrained('roberta-base')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:25:43.968441Z","iopub.execute_input":"2021-07-28T03:25:43.968764Z","iopub.status.idle":"2021-07-28T03:25:59.216153Z","shell.execute_reply.started":"2021-07-28T03:25:43.968738Z","shell.execute_reply":"2021-07-28T03:25:59.215110Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6aa72e035af74cb5a2098f4f874b15b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb1512414794423c97e681880400b84a"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('roberta-base')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:25:59.217824Z","iopub.execute_input":"2021-07-28T03:25:59.218195Z","iopub.status.idle":"2021-07-28T03:26:04.419381Z","shell.execute_reply.started":"2021-07-28T03:25:59.218155Z","shell.execute_reply":"2021-07-28T03:26:04.418335Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38c295819ccd477c91c6d84841bf9d90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0521b0e290a64c2bb5b8c7259d4231db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"167dec5f327c4a8d8bd50f26cb251bce"}},"metadata":{}}]},{"cell_type":"code","source":"RANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\n\ntrain = pd.read_csv('../input/paper-classcification/train.csv', sep='\\t')\ntest = pd.read_csv('../input/paper-classcification/test.csv', sep='\\t')\nsub = pd.read_csv('../input/paper-classcification/sample_submit.csv')\n\n# 拼接title与abstract\ntrain['text'] = train['title'] + ' ' + train['abstract']\ntest['text'] = test['title'] + ' ' + test['abstract']\n\nlabel_id2cate = dict(enumerate(train.categories.unique()))\nlabel_cate2id = {value: key for key, value in label_id2cate.items()}\n\ntrain['label'] = train['categories'].map(label_cate2id)\n\ntrain = train[['text', 'label']]\ntrain_y = train[\"label\"]\n# train_df = train[['text', 'label']][:45000]\n# eval_df = train[['text', 'label']][45000:]\n","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:26:04.420875Z","iopub.execute_input":"2021-07-28T03:26:04.421284Z","iopub.status.idle":"2021-07-28T03:26:05.964307Z","shell.execute_reply.started":"2021-07-28T03:26:04.421245Z","shell.execute_reply":"2021-07-28T03:26:05.963422Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"index = np.random.permutation(range(len(train)))\ntrain_data = train.iloc[index[:int(0.9*len(train))]].reset_index(drop = True)\nval_data = train.iloc[index[int(0.9*len(train)):]].reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:26:05.965608Z","iopub.execute_input":"2021-07-28T03:26:05.966056Z","iopub.status.idle":"2021-07-28T03:26:06.170461Z","shell.execute_reply.started":"2021-07-28T03:26:05.966021Z","shell.execute_reply":"2021-07-28T03:26:06.169437Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset,DataLoader,TensorDataset\nclass My_dataset(Dataset):\n    def __init__(self,data_len):\n        super(My_dataset,self).__init__()\n            \n        self.data = data_len\n    def __getitem__(self,idx):\n#         label = self.data.iloc[idx][2]\n#         mol = self.data.iloc[idx][0]\n#         pro = self.data.iloc[idx][1]\n#         pro = ' '.join(pro)\n#         mol_inputs = tokenizer_mol(mol)['input_ids']\n#         mol_inputs_mask = tokenizer_mol(mol)['attention_mask']\n#         pro_inputs = tokenizer_pro(pro)['input_ids']\n#         pro_inputs_mask = tokenizer_pro(pro)['attention_mask']\n#         pro_inputs_type = tokenizer_pro(pro)['token_type_ids']\n        \n        return self.data[idx]#mol,'_'.join(pro),label#torch.tensor(),torch.tensor(),torch.tensor(tss),torch.tensor(feats,dtype = torch.float32)\n    def __len__(self):\n        return len(self.data)\n#class my_dataset(nn.)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:26:06.171692Z","iopub.execute_input":"2021-07-28T03:26:06.172025Z","iopub.status.idle":"2021-07-28T03:26:06.179658Z","shell.execute_reply.started":"2021-07-28T03:26:06.171991Z","shell.execute_reply":"2021-07-28T03:26:06.178354Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"val_dataloader = DataLoader(list(range(len(val_data))),batch_size = 128,num_workers = 0,shuffle = True)\ntrain_dataloader = DataLoader(list(range(len(train_data))),batch_size = 128,num_workers = 0,shuffle = True)\ntest_dataloader = DataLoader(list(range(len(test))),batch_size = 128,num_workers = 0,shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:59:59.012297Z","iopub.execute_input":"2021-07-28T03:59:59.012614Z","iopub.status.idle":"2021-07-28T03:59:59.019429Z","shell.execute_reply.started":"2021-07-28T03:59:59.012586Z","shell.execute_reply":"2021-07-28T03:59:59.018610Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self,dim_in,dim_hidden,dim_out):\n        super(MLP,self).__init__()\n        self.fc_1 = nn.Linear(dim_in,dim_hidden)\n        self.fc_2 = nn.Linear(dim_hidden,dim_out)\n    def forward(self,data):\n        return self.fc_2(torch.relu(self.fc_1(data)))#return logits for crosss_entry","metadata":{"execution":{"iopub.status.busy":"2021-07-28T04:00:00.023298Z","iopub.execute_input":"2021-07-28T04:00:00.023631Z","iopub.status.idle":"2021-07-28T04:00:00.029324Z","shell.execute_reply.started":"2021-07-28T04:00:00.023602Z","shell.execute_reply":"2021-07-28T04:00:00.028215Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"modelmlp = MLP(768,768*2,39)\nLoss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(modelmlp.parameters(),lr = 0.001)\nepochs = 100\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nbest_acc = 0\npatience = 5\ncounter = 0\nfor epoch in range(1,epochs+1):\n    print('strating training!!')\n    train_loss = []\n    val_loss = []\n    #test_loss = 0\n    S = []\n    T = []\n    S_train = []\n    T_train = []\n    modelmlp.to(device)\n    modelmlp.train()\n    model.to(device)\n    model.eval()\n    pbar = tqdm(train_dataloader)\n    for batch in pbar:\n        batch_data = train_data.iloc[batch].copy()\n        #batch_data[1] = batch_data[1].apply(lambda x : re.sub(r\"[UZOB]\", \"X\", x))\n        # mol_batch = list(batch_data[0].values)\n        # pro_batch = list(batch_data[1].values)\n        text_batch = list(batch_data['text'].values)\n        label_batch = torch.tensor(list(batch_data['label'].values))\n#         model_mol.eval()\n#         model_pro.eval()\n        with torch.no_grad():\n            #size = [bs,768]\n            tokenizer_input = tokenizer(text_batch,return_tensors = 'pt',padding = True,max_length = 512,truncation = True)\n            input_ids,att_mask = tokenizer_input['input_ids'],tokenizer_input['attention_mask']\n            model_out = model(input_ids.to(device),att_mask.to(device))['pooler_output']\n            # #size = [bs,1024]\n            # pro_out = model_pro(**tokenizer_pro(pro_batch,return_tensors = 'pt',padding = True,max_length = 1500,truncation = True))['pooler_output']\n        # model_input = torch.cat([mol_out,pro_out],dim = 1)#bs,1792\n        model_out = modelmlp(model_out.to(device))\n        loss = Loss(model_out,label_batch.to(device))\n        scores = torch.softmax(model_out,dim = 1).cpu().detach().numpy()\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss.append(loss.item())\n        pbar.update()\n        pbar.set_description('train_loss: {}'.format(np.sum(train_loss)/len(train_loss)))\n        S_train.extend(scores)\n        T_train.extend(label_batch.cpu().detach().numpy())\n    AUC = roc_auc_score(T_train, S_train,multi_class='ovo')\n#     tpr, fpr, _ = precision_recall_curve(T_train, S_train)\n#     PRC = auc(fpr, tpr)\n    #print(AUC, PRC)\n    train_acc = np.sum(np.argmax(S_train,axis = 1) == T_train)/len(T_train)\n    print('train_loss:, ',np.sum(train_loss)/len(train_loss),'AUC: ',AUC,'train_acc: ',train_acc) \n    \n#val state\n    modelmlp.eval()\n    pbar = tqdm(val_dataloader)\n    for batch in pbar:\n\n        batch_data = val_data.iloc[batch].copy()\n        #batch_data[1] = batch_data[1].apply(lambda x : re.sub(r\"[UZOB]\", \"X\", x))\n        # mol_batch = list(batch_data[0].values)\n        # pro_batch = list(batch_data[1].values)\n        text_batch = list(batch_data['text'].values)\n        label_batch = torch.tensor(list(batch_data['label'].values))\n        with torch.no_grad():\n            #size = [bs,768]\n            # mol_out = model_mol(**tokenizer_mol(mol_batch,return_tensors = 'pt',padding = True,max_length = 512,truncation = True))['pooler_output']\n            #size = [bs,1024]\n            tokenizer_input = tokenizer(text_batch,return_tensors = 'pt',padding = True,max_length = 512,truncation = True)\n            input_ids,att_mask = tokenizer_input['input_ids'],tokenizer_input['attention_mask']\n            model_out = model(input_ids.to(device),att_mask.to(device))['pooler_output']\n            # model_input = torch.cat([mol_out,pro_out],dim = 1)#bs,1792\n            model_out = modelmlp(model_out.to(device))\n            loss = Loss(model_out,label_batch.to(device))\n            #scores = torch.softmax(model_out,dim = 1)[:,1]\n            val_loss.append(loss.item())\n            scores = torch.softmax(model_out,dim = 1).cpu().detach().numpy()\n            S.extend(scores)\n            T.extend(label_batch.cpu().detach().numpy())\n            pbar.set_description('val_loss: {}'.format(np.sum(val_loss)/len(val_loss)))\n            pbar.update()\n    AUC = roc_auc_score(T, S,multi_class='ovo')\n#     tpr, fpr, _ = precision_recall_curve(T, S)\n#     PRC = auc(fpr, tpr)\n    val_acc = np.sum(np.argmax(S,axis = 1) == T)/len(T)\n    counter +=1\n    print('val_loss: ',np.sum(val_loss)/len(val_loss),'AUC: ',AUC,'val_acc: ',val_acc) \n    if val_acc > best_acc:\n        counter = 0\n        best_acc = val_acc\n        torch.save({'model':modelmlp.state_dict(),\n                   'epoch':epoch,\n                   'optimizer':optimizer.state_dict(),\n                   'best_acc':best_acc,\n                   'val_loss':val_loss},'./my_pretrain_model.pth')\n    if counter > patience:\n        print('early stop in %d'%epoch)\n        break\n#test \nmodelmlp.eval()\npbar = tqdm(test_dataloader)\npredictions = []\nfor batch in pbar:\n\n    batch_data = test.iloc[batch].copy()\n    #batch_data[1] = batch_data[1].apply(lambda x : re.sub(r\"[UZOB]\", \"X\", x))\n    # mol_batch = list(batch_data[0].values)\n    # pro_batch = list(batch_data[1].values)\n    text_batch = list(batch_data['text'].values)\n    #label_batch = torch.tensor(list(batch_data['label'].values))\n    with torch.no_grad():\n        #size = [bs,768]\n        # mol_out = model_mol(**tokenizer_mol(mol_batch,return_tensors = 'pt',padding = True,max_length = 512,truncation = True))['pooler_output']\n        #size = [bs,1024]\n        tokenizer_input = tokenizer(text_batch,return_tensors = 'pt',padding = True,max_length = 512,truncation = True)\n        input_ids,att_mask = tokenizer_input['input_ids'],tokenizer_input['attention_mask']\n        model_out = model(input_ids.to(device),att_mask.to(device))['pooler_output']\n        # model_input = torch.cat([mol_out,pro_out],dim = 1)#bs,1792\n        model_out = modelmlp(model_out.to(device))\n#             loss = Loss(model_out,label_batch.to(device))\n        #scores = torch.softmax(model_out,dim = 1)[:,1]\n#         val_loss.append(loss.item())\n        scores = torch.softmax(model_out,dim = 1).cpu().detach().numpy()\n        prediction = np.argmax(scores,axis = 1)\n        predictions.extend(prediction)\n        pbar.update()\nsub['categories'] = predictions\nsub['categories'] = sub['categories'].map(label_id2cate)\nsub.to_csv('./submit.csv', index=False)\nprint('done')\n","metadata":{"execution":{"iopub.status.busy":"2021-07-28T04:00:11.932141Z","iopub.execute_input":"2021-07-28T04:00:11.932465Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"  0%|          | 0/352 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"strating training!!\n","output_type":"stream"},{"name":"stderr","text":"train_loss: 2.8612953647971153: 100%|██████████| 352/352 [12:43<00:00,  2.17s/it]\n  0%|          | 0/40 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"train_loss:,  2.8612953647971153 AUC:  0.5999713844268169 train_acc:  0.24322222222222223\n","output_type":"stream"},{"name":"stderr","text":"val_loss: 2.4830450773239137: 100%|██████████| 40/40 [01:27<00:00,  2.18s/it]\n  0%|          | 0/352 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"val_loss:  2.4830450773239137 AUC:  0.7359384212631351 val_acc:  0.2876\nstrating training!!\n","output_type":"stream"},{"name":"stderr","text":"train_loss: 2.1842042885043402: 100%|██████████| 352/352 [12:43<00:00,  2.17s/it]\n  0%|          | 0/40 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"train_loss:,  2.1842042885043402 AUC:  0.7956167143556877 train_acc:  0.40675555555555554\n","output_type":"stream"},{"name":"stderr","text":"val_loss: 1.9658739149570466: 100%|██████████| 40/40 [01:25<00:00,  2.15s/it]\n  0%|          | 0/352 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"val_loss:  1.9658739149570466 AUC:  0.8572823939953272 val_acc:  0.473\nstrating training!!\n","output_type":"stream"},{"name":"stderr","text":"train_loss: 1.8740962567654522: 100%|██████████| 352/352 [12:41<00:00,  2.16s/it]\n  0%|          | 0/40 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"train_loss:,  1.8740962567654522 AUC:  0.8542137263360708 train_acc:  0.4798222222222222\n","output_type":"stream"},{"name":"stderr","text":"val_loss: 1.7905518263578415: 100%|██████████| 40/40 [01:26<00:00,  2.17s/it]\n  0%|          | 0/352 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"val_loss:  1.7905518263578415 AUC:  0.8773920688274106 val_acc:  0.4946\nstrating training!!\n","output_type":"stream"},{"name":"stderr","text":"train_loss: 1.702981023625894: 100%|██████████| 352/352 [12:40<00:00,  2.16s/it] \n  0%|          | 0/40 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"train_loss:,  1.702981023625894 AUC:  0.8805506521891507 train_acc:  0.5238\n","output_type":"stream"},{"name":"stderr","text":"val_loss: 1.6299957454204559: 100%|██████████| 40/40 [01:26<00:00,  2.17s/it]\n  0%|          | 0/352 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"val_loss:  1.6299957454204559 AUC:  0.8974179397208656 val_acc:  0.5282\nstrating training!!\n","output_type":"stream"},{"name":"stderr","text":"train_loss: 1.58438524028117: 100%|██████████| 352/352 [12:47<00:00,  2.18s/it]  \n  0%|          | 0/40 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"train_loss:,  1.58438524028117 AUC:  0.8975075986693215 train_acc:  0.5537333333333333\n","output_type":"stream"},{"name":"stderr","text":"val_loss: 1.6097272634506226: 100%|██████████| 40/40 [01:27<00:00,  2.18s/it]\n  0%|          | 0/352 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"val_loss:  1.6097272634506226 AUC:  0.904878618528819 val_acc:  0.5338\nstrating training!!\n","output_type":"stream"},{"name":"stderr","text":"train_loss: 1.5085029944100163: 100%|██████████| 352/352 [12:45<00:00,  2.17s/it]\n  0%|          | 0/40 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"train_loss:,  1.5085029944100163 AUC:  0.9081334973671662 train_acc:  0.5714888888888889\n","output_type":"stream"},{"name":"stderr","text":"val_loss: 1.443980199098587: 100%|██████████| 40/40 [01:26<00:00,  2.15s/it] \n  0%|          | 0/352 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"val_loss:  1.443980199098587 AUC:  0.9244403615605042 val_acc:  0.5752\nstrating training!!\n","output_type":"stream"},{"name":"stderr","text":"train_loss: 1.4344554218378933: 100%|██████████| 352/352 [12:45<00:00,  2.17s/it]\n  0%|          | 0/40 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"train_loss:,  1.4344554218378933 AUC:  0.9179341185723494 train_acc:  0.5859777777777778\n","output_type":"stream"},{"name":"stderr","text":"val_loss: 1.3970359116792679: 100%|██████████| 40/40 [01:26<00:00,  2.15s/it]\n  0%|          | 0/352 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"val_loss:  1.3970359116792679 AUC:  0.9306838563683459 val_acc:  0.5892\nstrating training!!\n","output_type":"stream"},{"name":"stderr","text":"train_loss: 1.3995899178765037: 100%|██████████| 352/352 [12:44<00:00,  2.17s/it]\n  0%|          | 0/40 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"train_loss:,  1.3995899178765037 AUC:  0.9224821854835608 train_acc:  0.5950666666666666\n","output_type":"stream"},{"name":"stderr","text":"val_loss: 1.309428258375688:  38%|███▊      | 15/40 [00:23<00:47,  1.89s/it] ","output_type":"stream"}]},{"cell_type":"code","source":"torch.load('./my_pretrain_model.pth')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:54:23.209738Z","iopub.execute_input":"2021-07-28T03:54:23.210079Z","iopub.status.idle":"2021-07-28T03:54:23.268272Z","shell.execute_reply.started":"2021-07-28T03:54:23.210047Z","shell.execute_reply":"2021-07-28T03:54:23.267198Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"{'model': OrderedDict([('fc_1.weight',\n               tensor([[-0.0203, -0.0073,  0.0152,  ...,  0.0065, -0.0029,  0.0275],\n                       [-0.0146,  0.0135,  0.0076,  ..., -0.0159, -0.0069, -0.0150],\n                       [ 0.0072, -0.0293, -0.0082,  ...,  0.0340,  0.0354,  0.0096],\n                       ...,\n                       [-0.0028,  0.0049, -0.0280,  ..., -0.0045, -0.0193,  0.0233],\n                       [ 0.0094,  0.0129,  0.0210,  ..., -0.0098, -0.0001,  0.0292],\n                       [ 0.0035, -0.0009, -0.0239,  ...,  0.0039,  0.0044,  0.0027]],\n                      device='cuda:0')),\n              ('fc_1.bias',\n               tensor([ 0.0277, -0.0112,  0.0122,  ..., -0.0228,  0.0244, -0.0321],\n                      device='cuda:0')),\n              ('fc_2.weight',\n               tensor([[ 0.0175,  0.0259, -0.0117,  ...,  0.0216, -0.0104,  0.0215],\n                       [-0.0250, -0.0195, -0.0165,  ...,  0.0144,  0.0197,  0.0213],\n                       [ 0.0074,  0.0084, -0.0290,  ...,  0.0016, -0.0087,  0.0104],\n                       ...,\n                       [ 0.0007, -0.0283,  0.0162,  ..., -0.0101, -0.0172,  0.0127],\n                       [ 0.0145, -0.0220, -0.0030,  ...,  0.0132, -0.0077, -0.0099],\n                       [ 0.0199, -0.0021, -0.0108,  ...,  0.0201, -0.0146,  0.0200]],\n                      device='cuda:0')),\n              ('fc_2.bias',\n               tensor([ 0.0043, -0.0105, -0.0241, -0.0109,  0.0197,  0.0164, -0.0138, -0.0115,\n                       -0.0064,  0.0272, -0.0136, -0.0171,  0.0008, -0.0021, -0.0256,  0.0008,\n                        0.0054, -0.0097, -0.0155, -0.0002,  0.0076,  0.0208, -0.0272,  0.0043,\n                        0.0188, -0.0010, -0.0062,  0.0120,  0.0213,  0.0241, -0.0083, -0.0153,\n                        0.0083,  0.0164, -0.0317, -0.0003, -0.0144, -0.0008, -0.0296],\n                      device='cuda:0'))]),\n 'epoch': 1,\n 'optimizer': {'state': {0: {'step': 8,\n    'exp_avg': tensor([[-1.2643e-08, -3.9182e-07, -3.8958e-07,  ..., -3.3194e-07,\n             -1.1586e-07, -1.8536e-07],\n            [ 7.4373e-06, -1.8218e-05, -9.3778e-06,  ..., -1.0860e-05,\n             -8.2003e-07,  9.2064e-06],\n            [-1.9725e-06, -4.7251e-05, -4.3956e-05,  ..., -3.5510e-05,\n             -1.2421e-05, -2.2054e-05],\n            ...,\n            [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n              0.0000e+00,  0.0000e+00],\n            [-2.3889e-06,  1.1095e-04,  8.7035e-05,  ...,  6.7239e-05,\n              2.6318e-05,  4.7473e-05],\n            [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n              0.0000e+00,  0.0000e+00]], device='cuda:0'),\n    'exp_avg_sq': tensor([[6.9389e-17, 6.6642e-14, 6.5882e-14,  ..., 4.7829e-14, 5.8268e-15,\n             1.4914e-14],\n            [5.5322e-12, 1.4724e-09, 1.2447e-09,  ..., 8.0370e-10, 8.3334e-11,\n             3.6092e-10],\n            [1.6889e-12, 9.6914e-10, 8.3870e-10,  ..., 5.4735e-10, 6.6965e-11,\n             2.1112e-10],\n            ...,\n            [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n             0.0000e+00],\n            [4.3230e-12, 1.2351e-09, 9.6615e-10,  ..., 6.3515e-10, 7.4775e-11,\n             2.6376e-10],\n            [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n             0.0000e+00]], device='cuda:0')},\n   1: {'step': 8,\n    'exp_avg': tensor([ 1.8670e-06,  4.6188e-05,  2.2648e-04,  ...,  0.0000e+00,\n            -4.6170e-04,  0.0000e+00], device='cuda:0'),\n    'exp_avg_sq': tensor([1.5130e-12, 3.1995e-08, 2.2264e-08,  ..., 0.0000e+00, 2.4863e-08,\n            0.0000e+00], device='cuda:0')},\n   2: {'step': 8,\n    'exp_avg': tensor([[ 1.9839e-08,  2.0425e-03, -2.0049e-04,  ...,  0.0000e+00,\n              1.9095e-03,  0.0000e+00],\n            [ 1.8384e-08,  3.4344e-04,  4.4786e-05,  ...,  0.0000e+00,\n              4.8333e-04,  0.0000e+00],\n            [ 1.8215e-08, -1.1117e-03,  4.0251e-05,  ...,  0.0000e+00,\n             -1.6960e-03,  0.0000e+00],\n            ...,\n            [ 2.0769e-08,  1.0042e-03,  5.6633e-05,  ...,  0.0000e+00,\n              1.1876e-03,  0.0000e+00],\n            [ 1.8296e-08,  6.2600e-04,  1.1543e-04,  ...,  0.0000e+00,\n              7.5023e-04,  0.0000e+00],\n            [ 1.7697e-08,  5.6235e-04,  1.1210e-04,  ...,  0.0000e+00,\n              8.6986e-04,  0.0000e+00]], device='cuda:0'),\n    'exp_avg_sq': tensor([[1.7085e-16, 7.3413e-07, 1.7449e-08,  ..., 0.0000e+00, 1.3853e-06,\n             0.0000e+00],\n            [1.4670e-16, 3.5948e-08, 8.7065e-10,  ..., 0.0000e+00, 9.3219e-08,\n             0.0000e+00],\n            [1.4402e-16, 1.4861e-07, 7.0325e-10,  ..., 0.0000e+00, 3.5644e-07,\n             0.0000e+00],\n            ...,\n            [1.8723e-16, 4.8587e-08, 1.3922e-09,  ..., 0.0000e+00, 9.4654e-08,\n             0.0000e+00],\n            [1.4530e-16, 5.0903e-08, 5.7840e-09,  ..., 0.0000e+00, 1.3664e-07,\n             0.0000e+00],\n            [1.3595e-16, 1.1475e-08, 5.4545e-09,  ..., 0.0000e+00, 2.7414e-08,\n             0.0000e+00]], device='cuda:0')},\n   3: {'step': 8,\n    'exp_avg': tensor([ 1.0865e-04,  1.1035e-03, -2.2238e-03, -4.4105e-04, -2.0976e-03,\n             1.4756e-03, -2.1395e-03, -5.1912e-03,  9.1370e-04, -1.9515e-03,\n             1.9419e-03, -3.9229e-03, -5.3512e-05, -1.2332e-03,  9.7549e-04,\n            -7.2137e-03,  1.5421e-03,  1.7711e-03, -2.9484e-03, -5.3726e-04,\n             9.8690e-04, -5.5257e-03,  2.9174e-03, -1.2723e-03,  3.4511e-03,\n             2.5559e-03,  9.3971e-04, -3.4856e-03, -1.0519e-02, -1.5762e-03,\n             1.7895e-03,  3.8329e-03,  3.6544e-03,  4.9538e-03,  4.6919e-03,\n             1.9103e-03,  3.7158e-03,  3.6429e-03,  3.4573e-03], device='cuda:0'),\n    'exp_avg_sq': tensor([7.3571e-06, 6.5551e-07, 1.6992e-06, 1.4991e-04, 4.7154e-07, 6.6027e-06,\n            1.6559e-06, 3.0236e-06, 4.4431e-07, 6.2114e-06, 9.5976e-07, 3.3596e-06,\n            2.3145e-06, 8.2455e-07, 1.2172e-06, 3.1270e-06, 2.5480e-06, 1.1207e-06,\n            2.2133e-06, 1.7837e-06, 2.5089e-07, 2.5478e-06, 1.4912e-06, 1.7254e-06,\n            1.3408e-06, 4.7700e-07, 6.9993e-07, 1.6476e-06, 9.0587e-06, 1.5697e-06,\n            1.1179e-06, 1.1570e-06, 1.2148e-06, 1.4931e-06, 1.4649e-06, 1.1603e-06,\n            1.0809e-06, 1.8055e-06, 8.5939e-07], device='cuda:0')}},\n  'param_groups': [{'lr': 0.001,\n    'betas': (0.9, 0.999),\n    'eps': 1e-08,\n    'weight_decay': 0,\n    'amsgrad': False,\n    'params': [0, 1, 2, 3]}]},\n 'best_acc': 0.226,\n 'val_loss': [3.0907654762268066,\n  3.2032108306884766,\n  3.2311031818389893,\n  2.922602653503418,\n  3.2880747318267822,\n  3.2807366847991943,\n  3.1352059841156006,\n  3.056633234024048]}"},"metadata":{}}]},{"cell_type":"code","source":"pd.read_csv('./submit.csv')['categories'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:56:31.134098Z","iopub.execute_input":"2021-07-28T03:56:31.134420Z","iopub.status.idle":"2021-07-28T03:56:31.149507Z","shell.execute_reply.started":"2021-07-28T03:56:31.134391Z","shell.execute_reply":"2021-07-28T03:56:31.148571Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"array(['cs.CV'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"y_true = [1,2,3,4,5,6,7,8]","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:33:24.573333Z","iopub.status.idle":"2021-07-28T03:33:24.573889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby('label').size()/len(train)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:59:27.892672Z","iopub.execute_input":"2021-07-28T03:59:27.893017Z","iopub.status.idle":"2021-07-28T03:59:27.905733Z","shell.execute_reply.started":"2021-07-28T03:59:27.892983Z","shell.execute_reply":"2021-07-28T03:59:27.904616Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"label\n0     0.08520\n1     0.01408\n2     0.01074\n3     0.22076\n4     0.02704\n5     0.05018\n6     0.01540\n7     0.03768\n8     0.01046\n9     0.05596\n10    0.00726\n11    0.06436\n12    0.05412\n13    0.03880\n14    0.01366\n15    0.03482\n16    0.02584\n17    0.00628\n18    0.01682\n19    0.01206\n20    0.01354\n21    0.01886\n22    0.00420\n23    0.01968\n24    0.00460\n25    0.00938\n26    0.01438\n27    0.01996\n28    0.03988\n29    0.02456\n30    0.00724\n31    0.00522\n32    0.00352\n33    0.00278\n34    0.00198\n35    0.00280\n36    0.00344\n37    0.00210\n38    0.00036\ndtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"sub","metadata":{"execution":{"iopub.status.busy":"2021-07-28T03:27:36.265688Z","iopub.execute_input":"2021-07-28T03:27:36.266026Z","iopub.status.idle":"2021-07-28T03:27:36.283422Z","shell.execute_reply.started":"2021-07-28T03:27:36.265996Z","shell.execute_reply":"2021-07-28T03:27:36.282532Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"         paperid categories\n0     test_00000      cs.CV\n1     test_00001      cs.CV\n2     test_00002      cs.CV\n3     test_00003      cs.CV\n4     test_00004      cs.CV\n...          ...        ...\n9995  test_09995      cs.CV\n9996  test_09996      cs.CV\n9997  test_09997      cs.CV\n9998  test_09998      cs.CV\n9999  test_09999      cs.CV\n\n[10000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>paperid</th>\n      <th>categories</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>test_00000</td>\n      <td>cs.CV</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>test_00001</td>\n      <td>cs.CV</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>test_00002</td>\n      <td>cs.CV</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>test_00003</td>\n      <td>cs.CV</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>test_00004</td>\n      <td>cs.CV</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>test_09995</td>\n      <td>cs.CV</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>test_09996</td>\n      <td>cs.CV</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>test_09997</td>\n      <td>cs.CV</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>test_09998</td>\n      <td>cs.CV</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>test_09999</td>\n      <td>cs.CV</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}